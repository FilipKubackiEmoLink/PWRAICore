{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f228d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from rich import print as rprint  # Import rich's print with a distinct name\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "def pp(data: Any, limit: int = 5, title: str | None = None):\n",
    "    \"\"\"\n",
    "    Intelligently pretty-prints any Python object with color and a limit for collections.\n",
    "\n",
    "    - For dictionaries and lists, it shows the first `limit` items.\n",
    "    - For other types, it prints the object directly.\n",
    "    - Uses the 'rich' library for beautiful, syntax-highlighted output.\n",
    "\n",
    "    Args:\n",
    "        data: The Python object to print (dict, list, str, int, etc.).\n",
    "        limit: The maximum number of items to display for collections.\n",
    "        title: An optional title for the output header.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Print a clear, consistent title ---\n",
    "    if title:\n",
    "        rprint(f\"[bold inverse cyan] {title} [/bold inverse cyan]\")\n",
    "\n",
    "    # === Case 1: Handle Dictionaries ===\n",
    "    if isinstance(data, dict):\n",
    "        total_items = len(data)\n",
    "        if total_items > limit:\n",
    "            rprint(f\"--- Displaying the first [bold]{limit}[/bold] of [bold]{total_items}[/bold] dictionary items ---\")\n",
    "            # Use islice for a memory-efficient slice of the dictionary's items\n",
    "            display_obj = dict(islice(data.items(), limit))\n",
    "        else:\n",
    "            rprint(f\"--- Displaying all [bold]{total_items}[/bold] dictionary items ---\")\n",
    "            display_obj = data\n",
    "        rprint(display_obj)\n",
    "\n",
    "    # === Case 2: Handle Lists, Tuples, Sets (Sequences) ===\n",
    "    elif isinstance(data, (list, tuple, set)):\n",
    "        total_items = len(data)\n",
    "        if total_items > limit:\n",
    "            rprint(f\"--- Displaying the first [bold]{limit}[/bold] of [bold]{total_items}[/bold] items ---\")\n",
    "            # islice also works perfectly on lists and other iterables\n",
    "            display_obj = list(islice(data, limit))\n",
    "        else:\n",
    "            rprint(f\"--- Displaying all [bold]{total_items}[/bold] items ---\")\n",
    "            display_obj = data\n",
    "        rprint(display_obj)\n",
    "        \n",
    "    # === Case 3: Handle all other data types ===\n",
    "    else:\n",
    "        rprint(f\"--- Displaying object of type: [bold]{type(data).__name__}[/bold] ---\")\n",
    "        rprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d85c7a7",
   "metadata": {},
   "source": [
    "## üìì Notebooks: Enabling Interactive `tqdm` Progress Bars\n",
    "\n",
    "When working in Jupyter notebooks, you'll want to see clean, interactive progress bars for long-running tasks. However, the standard setup can lead to a couple of common problems. This guide shows you the professional way to fix it.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "You might run into one of two issues:\n",
    "\n",
    "1.  **Messy, multi-line output:** Using the standard `from tqdm import tqdm` will print a new progress bar on every update, flooding your output cell.\n",
    "2.  **An `ImportError`:** Following the best practice of using `from tqdm.notebook import tqdm` might give you this error:\n",
    "    ```\n",
    "    ImportError: IProgress not found. Please update jupyter and ipywidgets.\n",
    "    ```\n",
    "\n",
    "This happens because the beautiful, interactive notebook widgets require a special library (`ipywidgets`) and a Jupyter extension that aren't installed by default.\n",
    "\n",
    "### The Solution: Install and Enable `ipywidgets`\n",
    "\n",
    "Follow these steps in your terminal at the root of the project to permanently fix this.\n",
    "\n",
    "#### Step 1: Add `ipywidgets` as a Dev Dependency\n",
    "\n",
    "The `ipywidgets` library is a tool for your development environment, not a core dependency of the final application. We'll use the `--dev` flag to add it correctly.\n",
    "\n",
    "```bash\n",
    "rye add --dev ipywidgets\n",
    "rye sync\n",
    "```\n",
    "\n",
    "#### Step 3: Enable the Jupyter Extension\n",
    "\n",
    "Installing the package isn't enough. You need to tell Jupyter to activate its interactive components. We use rye run to ensure the command executes within our project's managed environment.\n",
    "\n",
    "```bash\n",
    "rye run jupyter nbextension enable --py widgetsnbextension\n",
    "```\n",
    "\n",
    "#### Step 4: Restart Your Jupyter Kernel! üîÑ\n",
    "\n",
    "This is the most important step! Your currently running notebook doesn't know about the new extension yet.\n",
    "In your notebook interface (e.g., VS Code or Jupyter Lab), click Kernel > Restart Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# Configure professional logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "# --- Change 2: Create a custom handler to integrate logging with tqdm ---\n",
    "# This ensures log messages don't break the progress bar.\n",
    "class TqdmLoggingHandler(logging.Handler):\n",
    "    def __init__(self, level=logging.NOTSET):\n",
    "        super().__init__(level)\n",
    "\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            msg = self.format(record)\n",
    "            # Use tqdm's built-in safe writer\n",
    "            tqdm.write(msg)\n",
    "            self.flush()\n",
    "        except Exception:\n",
    "            self.handleError(record)\n",
    "\n",
    "# --- Configure Logging to use our new handler ---\n",
    "# We remove the basicConfig and set up the handler manually\n",
    "log = logging.getLogger()\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "# Clear any existing handlers to avoid duplicate messages\n",
    "if log.hasHandlers():\n",
    "    log.handlers.clear()\n",
    "\n",
    "# Add our custom tqdm handler\n",
    "log.addHandler(TqdmLoggingHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd14e73",
   "metadata": {},
   "source": [
    "## üìÇ Phase 2a: Data Preparation - Unzipping the Archives\n",
    "\n",
    "Before we can analyze the financial documents, we need to extract them from their compressed archives. This section details the initial data preparation script which unzips all the source files and creates a master index for our project.\n",
    "\n",
    "### The Goal\n",
    "\n",
    "The source data is provided as a directory full of `.zip` files. Each zip file is named after a company's KRS number (e.g., `0000001132.zip`) and contains one or more financial documents for that company.\n",
    "\n",
    "The purpose of this script is to:\n",
    "1.  Scan a specified directory for all `.zip` archives.\n",
    "2.  Unzip each archive into its own dedicated folder to keep the data organized.\n",
    "3.  Create a comprehensive JSON lookup map, `file_map.json`, that connects each KRS number to the list of all file paths that were extracted for it.\n",
    "\n",
    "### How the Script Works\n",
    "\n",
    "The script (`your_script_name.py`) performs the following actions:\n",
    "\n",
    "1.  **Sets the Source Path:** It starts with a `Path` variable that you must configure to point to your data directory:\n",
    "    ```python\n",
    "    ZIP_DATA_PATH = Path(\"/path/to/your/zip/files-directory/\")\n",
    "    ```\n",
    "\n",
    "2.  **Scans for Zips:** It walks through the `ZIP_DATA_PATH` and identifies all files ending with `.zip`.\n",
    "\n",
    "3.  **Extracts in a Loop:** It iterates through each zip file one by one. For a file named `0000001132.zip`, it will:\n",
    "    *   Create a new directory named `0000001132/` in the same location.\n",
    "    *   Extract all the contents of the zip file into this new directory.\n",
    "\n",
    "4.  **Builds the Map:** During the loop, it constructs a Python dictionary. The key is the KRS number (derived from the zip filename, e.g., `\"0000001132\"`), and the value is a list of the full paths to every file that was just extracted.\n",
    "\n",
    "5.  **Saves the Output:** Once all zip files have been processed, the script saves the entire dictionary to a file named `file_map.json` in the project's root directory. This JSON file will be the primary input for the next phase of our project.\n",
    "\n",
    "### üöÄ Performance Enhancement: A Task for You! (TODO)\n",
    "\n",
    "The current script processes the zip files sequentially, one after another. This is simple and reliable, but it can be slow if you have hundreds or thousands of files.\n",
    "\n",
    "This task‚Äîunzipping files‚Äîis what's known as an **\"embarrassingly parallel\" problem**. Each zip file can be processed completely independently of the others. This makes it a perfect candidate for **multiprocessing**.\n",
    "\n",
    "**Your Challenge:**\n",
    "Refactor the `unzip_and_map_files` function to use Python's `multiprocessing` module (specifically, a `Pool` of workers) to unzip the files in parallel. This will dramatically speed up the data preparation step, especially on multi-core machines.\n",
    "\n",
    "**Hints:**\n",
    "*   You'll want to use `Pool.map()` or `Pool.imap_unordered()`.\n",
    "*   The function that the pool executes should handle a single zip file (`_unzip_single_file` is already a great starting point).\n",
    "*   Think about how you will collect the results from all the parallel processes to build the final `directory_file_map` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452d325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "\n",
    "\n",
    "def _unzip_single_file(\n",
    "    zip_path: Path,\n",
    ") -> Optional[Tuple[Path, List[Path]]]:\n",
    "    \"\"\"\n",
    "    Unzips a single zip file into a directory named after the zip file.\n",
    "\n",
    "    For example, 'my_files.zip' will be extracted to a new 'my_files/' directory\n",
    "    in the same location.\n",
    "\n",
    "    Args:\n",
    "        zip_path: The path to the .zip file.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - The path to the new destination directory.\n",
    "        - A list of paths to all the files inside the new directory.\n",
    "        Returns None if an error occurs.\n",
    "    \"\"\"\n",
    "    # Create the destination directory path (e.g., /path/to/file.zip -> /path/to/file)\n",
    "    destination_dir = zip_path.with_suffix('')\n",
    "    \n",
    "    try:\n",
    "        # Create the directory; exist_ok=True prevents errors if it already exists\n",
    "        destination_dir.mkdir(parents=True, exist_ok=True)\n",
    "        # logging.info(f\"Extracting '{zip_path.name}' to '{destination_dir.name}/'\")\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(destination_dir)\n",
    "            \n",
    "        # After extraction, find all *files* inside the new directory (recursively)\n",
    "        # We use rglob to find files in subdirectories too.\n",
    "        extracted_files = [p for p in destination_dir.rglob('*') if p.is_file()]\n",
    "        \n",
    "        return extracted_files\n",
    "        \n",
    "    except zipfile.BadZipFile:\n",
    "        logging.error(f\"Error: '{zip_path.name}' is not a valid or is a corrupted zip file. Skipping.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred with '{zip_path.name}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def unzip_and_map_files(\n",
    "    zip_data_path: Path,\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Finds all .zip files in a directory, unzips each one into its own folder,\n",
    "    and returns a dictionary mapping each new folder to the files inside it.\n",
    "\n",
    "    Args:\n",
    "        zip_data_path: The path to the directory containing the .zip files.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are the paths of the new unzipped directories\n",
    "        and values are lists of the file paths within those directories.\n",
    "    \"\"\"\n",
    "    if not zip_data_path.is_dir():\n",
    "        logging.error(f\"Error: Provided path is not a directory: {zip_data_path}\")\n",
    "        return {}\n",
    "    \n",
    "    # Find all .zip files in the directory\n",
    "    zip_files = list(zip_data_path.glob(\"*.zip\"))\n",
    "    if not zip_files:\n",
    "        logging.warning(f\"No .zip files found in '{zip_data_path}'\")\n",
    "        return {}\n",
    "    \n",
    "    pp(zip_files, title=\"Zip Files Found\", limit=5)\n",
    "\n",
    "    logging.info(f\"Found {len(zip_files)} zip files to process.\")\n",
    "    \n",
    "    # The final dictionary to hold our results\n",
    "    directory_file_map: Dict[str, List[str]] = {}\n",
    "    \n",
    "    for zip_path in tqdm(zip_files, desc=\"Unzipping files\"):\n",
    "        result = _unzip_single_file(zip_path)\n",
    "        krs_number = zip_path.stem\n",
    "        \n",
    "        # Only add to the map if unzipping was successful\n",
    "        if result:\n",
    "            file_list = result\n",
    "            # We store paths as strings for easier serialization (e.g., to JSON)\n",
    "            directory_file_map[krs_number] = [str(f) for f in file_list]\n",
    "            \n",
    "    return directory_file_map\n",
    "\n",
    "ZIP_DATA_PATH = Path(\"/mnt/e/Workspace/PKDData/2Download/FinanceKRS_Sampled_PWR/\")\n",
    "file_map = unzip_and_map_files(ZIP_DATA_PATH)\n",
    "\n",
    "#! add file_map.json to .gitignore - you do not want to commit this file\n",
    "FILE_MAP = \"file_map.json\"\n",
    "logging.info(f\"Found {len(file_map)} directories after unzipping. Generating lookup map {FILE_MAP} at the root level.\")\n",
    "with open(f\"../{FILE_MAP}\", \"w\") as f:\n",
    "    json.dump(file_map, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e19b06",
   "metadata": {},
   "source": [
    "## üìä Phase 2b: Extracting and Parsing PDF Tables\n",
    "\n",
    "This is the core data engineering phase of our project. Here, we take the file paths from our prepared data map and tackle the significant challenge of extracting structured, clean tables from unstructured PDF documents.\n",
    "\n",
    "The process is broken down into a clear, multi-step pipeline for each PDF file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81c8fbb",
   "metadata": {},
   "source": [
    "### Step 1: Loading the File Map\n",
    "\n",
    "The entire process begins with the master index we created in the previous step, `file_map.json`. This file is our single source of truth for the locations of all financial documents.\n",
    "\n",
    "The first action in our script or notebook is to load this map and convert the string paths back into `pathlib.Path` objects, which are easier and safer to work with in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde43ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_MAP = \"file_map.json\"\n",
    "with open(f\"../{FILE_MAP}\", \"r\") as f:\n",
    "    file_map = json.load(f)\n",
    "# convert string paths back to Path objects\n",
    "file_map = {k: [Path(p) for p in v] for k, v in file_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322b579d",
   "metadata": {},
   "source": [
    "### Step 2: The Table Extraction and Parsing Pipeline\n",
    "\n",
    "For any given PDF, we apply a sophisticated three-stage process to transform its visual tables into clean, usable data.\n",
    "\n",
    "#### A. Raw Extraction (`extract_raw_tables_from_pdf`)\n",
    "\n",
    "This is the first-pass extraction.\n",
    "\n",
    "*   **Input:** A single `Path` object pointing to a PDF file.\n",
    "*   **Tool:** It uses the `pdfplumber` library to open the PDF and scan each page for anything that looks like a table.\n",
    "*   **Output:** A list of \"raw\" tables. This output is often messy and reflects the challenges of PDF parsing:\n",
    "    *   Columns are frequently merged into a single cell.\n",
    "    *   A single logical row might be split across multiple lines (`\\n`) within one cell.\n",
    "    *   The data is not yet structured or cleaned.\n",
    "\n",
    "#### B. Displaying the Results (`display_cleaned_tables`)\n",
    "\n",
    "The final function is for visualization and validation.\n",
    "\n",
    "*   **Input:** The list of all cleaned tables extracted from the PDF.\n",
    "*   **Tool:** It uses the powerful `rich` library to render the clean data.\n",
    "*   **Output:** A series of beautifully formatted, human-readable tables printed to the console or notebook output. This allows us to immediately inspect the results and verify if the parsing logic was successful for a given document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0cb5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pdfplumber\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "console = Console()\n",
    "ExtractedTable = List[List[Optional[str]]]\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path: Path) -> List[ExtractedTable]:\n",
    "    \"\"\"\n",
    "    Extracts all tables from a single PDF file using pdfplumber.\n",
    "\n",
    "    Args:\n",
    "        pdf_path: The file path to the PDF document.\n",
    "\n",
    "    Returns:\n",
    "        A list of tables found in the document. Each table is a 2D list\n",
    "        of strings. Returns an empty list if no tables are found or if an error occurs.\n",
    "    \"\"\"\n",
    "    if not pdf_path.is_file():\n",
    "        logging.error(f\"File not found or is not a file: {pdf_path}\")\n",
    "        return []\n",
    "\n",
    "    tables: List[ExtractedTable] = []\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # We iterate through each page and extract tables\n",
    "            for page in pdf.pages:\n",
    "                # .extract_tables() is a robust method that finds all tables on a page\n",
    "                extracted_page_tables = page.extract_tables()\n",
    "                if extracted_page_tables:\n",
    "                    tables.extend(extracted_page_tables)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not process PDF file '{pdf_path.name}'. Reason: {e}\")\n",
    "        return []\n",
    "\n",
    "    return tables\n",
    "\n",
    "\n",
    "def display_tables(tables: List[ExtractedTable]):\n",
    "    \"\"\"Uses rich to render a list of tables in the console.\"\"\"\n",
    "    if not tables:\n",
    "        console.print(\"[yellow]No tables were found in this document.[/yellow]\")\n",
    "        return\n",
    "\n",
    "    console.print(f\"[bold green]Successfully extracted {len(tables)} tables.[/bold green]\\n\")\n",
    "\n",
    "    for i, table_data in enumerate(tables):\n",
    "        if not table_data:\n",
    "            continue\n",
    "\n",
    "        # Create a rich Table object for beautiful printing\n",
    "        table_display = Table(title=f\"Table #{i + 1}\", show_header=True, header_style=\"bold magenta\")\n",
    "        \n",
    "        # Assume the first row is the header\n",
    "        header = table_data[0]\n",
    "        # Clean header: handle None and convert all to string\n",
    "        cleaned_header = [str(h) if h is not None else f\"Col_{j}\" for j, h in enumerate(header)]\n",
    "\n",
    "        for column_header in cleaned_header:\n",
    "            table_display.add_column(column_header)\n",
    "        \n",
    "        # Add the data rows\n",
    "        for row in table_data[1:]:\n",
    "            # Clean row: convert all cells to string to satisfy rich Table\n",
    "            cleaned_row = [str(cell) if cell is not None else \"\" for cell in row]\n",
    "            table_display.add_row(*cleaned_row)\n",
    "        \n",
    "        console.print(table_display)\n",
    "        console.print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# pick  first pdf from the file_map for testing\n",
    "# it's dummy code for testing purposes \n",
    "# TODO: add check if the pdf_paths are really pdf files\n",
    "pdf_paths = next(iter(file_map.values()))\n",
    "tables = extract_tables_from_pdf(pdf_paths[0])  # Example usage with the first PDF path\n",
    "display_tables(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b3569a",
   "metadata": {},
   "source": [
    "### üí• KNOCKDOWN: Your First Real-World Encounter\n",
    "\n",
    "So, you've built a parser. You've seen it work on one, maybe two, documents. You've cleaned up a messy table and felt the satisfaction of turning chaos into order.\n",
    "\n",
    "Now, it's time for a dose of reality.\n",
    "\n",
    "![A person hitting a brick wall](https://i.etsystatic.com/13687543/r/il/433c6a/1098404604/il_570xN.1098404604_eiv9.jpg)\n",
    "\n",
    "Take a moment. Go into the `data/` directory and open one of the more complex, multi-page PDFs *manually*. Don't run any code. Just scroll through it. Look at the tables that are split across pages. Notice the footnotes, the weirdly merged headers, the inconsistent column spacing.\n",
    "\n",
    "Now ask yourself: **Do you really believe it's worth cleaning this mess from this point onward?**\n",
    "\n",
    "The script you wrote is an amazing learning tool, but in the real world, you won't be dealing with ten documents. You'll have **thousands**, each one formatted with its own unique, frustrating quirks. Building a custom parser for every single variation is not just difficult; it's an unwinnable battle.\n",
    "\n",
    "This is a critical lesson in software engineering. A great engineer doesn't just solve the problem; they find the *right tool* for the problem. Your custom parser was an excellent exercise in understanding the deep complexity of the task.\n",
    "\n",
    "Now, it's time to go to the internet and find a better tool. Your next mission is to research and evaluate existing solutions‚Äîfrom open-source libraries to cloud-based AI services‚Äîthat are specifically designed to handle this level of document complexity at scale.\n",
    "\n",
    "Good luck.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2584a9",
   "metadata": {},
   "source": [
    "### Solution to multiprocessing\n",
    "\n",
    "ADVANCED: Why do you think we had to create another function unzip_worker? It's so dummy, check what sort of error you will see replacing the line:\n",
    "```bash\n",
    "results_iterator = pool.imap_unordered(unzip_worker, zip_files)\n",
    "\n",
    "with:\n",
    "\n",
    "results_iterator = pool.imap_unordered(lambda zip_path: (zip_path.stem, _unzip_single_file(zip_path)), zip_files)\n",
    "```\n",
    "\n",
    "Can you write lambda function on your own?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca192e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def unzip_worker(zip_path: Path) -> Optional[Tuple[str, List[str]]]:\n",
    "    return (zip_path.stem, _unzip_single_file(zip_path))\n",
    "\n",
    "def unzip_and_map_files_parallel(\n",
    "    zip_data_path: Path,\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Finds all .zip files in a directory, unzips each one into its own folder,\n",
    "    and returns a dictionary mapping each new folder to the files inside it.\n",
    "\n",
    "    Args:\n",
    "        zip_data_path: The path to the directory containing the .zip files.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are the paths of the new unzipped directories\n",
    "        and values are lists of the file paths within those directories.\n",
    "    \"\"\"\n",
    "    if not zip_data_path.is_dir():\n",
    "        logging.error(f\"Error: Provided path is not a directory: {zip_data_path}\")\n",
    "        return {}\n",
    "    \n",
    "    # Find all .zip files in the directory\n",
    "    zip_files = list(zip_data_path.glob(\"*.zip\"))\n",
    "    if not zip_files:\n",
    "        logging.warning(f\"No .zip files found in '{zip_data_path}'\")\n",
    "        return {}\n",
    "    \n",
    "    pp(zip_files, title=\"Zip Files Found\", limit=5)\n",
    "\n",
    "    logging.info(f\"Found {len(zip_files)} zip files to process.\")\n",
    "    \n",
    "    # The final dictionary to hold our results\n",
    "    directory_file_map: Dict[str, List[str]] = {}\n",
    "    # ==============================================================================\n",
    "    # Change 2: Replace the for loop with a multiprocessing Pool\n",
    "    # ==============================================================================\n",
    "    # Use all available CPU cores for maximum speed\n",
    "    num_processes = cpu_count()\n",
    "\n",
    "    # The 'with' statement ensures the pool of processes is properly closed\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        # Use pool.imap_unordered for efficiency. It applies the worker function\n",
    "        # to each item in zip_files and yields results as they are completed.\n",
    "        # This allows the tqdm progress bar to update in real-time.\n",
    "        \n",
    "        # We wrap the imap_unordered call with tqdm to create the progress bar.\n",
    "        # `total=len(zip_files)` tells tqdm the total number of tasks.\n",
    "        results_iterator = pool.imap_unordered(unzip_worker, zip_files)\n",
    "        \n",
    "        for result in tqdm(results_iterator, total=len(zip_files), desc=\"Unzipping files\"):\n",
    "            if result:\n",
    "                directory_file_map[result[0]] = result[1]\n",
    "                # results.append(result)\n",
    "    logging.info(\"Parallel processing complete. Assembling final map.\")\n",
    "\n",
    "    # ==============================================================================\n",
    "    # Change 3: Efficiently convert the list of (key, value) tuples into a dictionary\n",
    "    # ==============================================================================\n",
    "    return directory_file_map\n",
    "\n",
    "ZIP_DATA_PATH = Path(\"/mnt/e/Workspace/PKDData/2Download/FinanceKRS_Sampled_PWR/\")\n",
    "file_map = unzip_and_map_files_parallel(ZIP_DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d921a69",
   "metadata": {},
   "source": [
    "### The `multiprocessing` and `lambda` Rule\n",
    "\n",
    "A common pitfall in Python's `multiprocessing` is the `AttributeError: Can't get local object '<locals>.<lambda>'`. This error occurs when you try to pass a `lambda` function to a worker pool. Here is the compact explanation of why this happens.\n",
    "\n",
    "**The \"Why\" in one sentence:** A `lambda` function cannot be sent to another process because it is **anonymous** and **local**; a worker process needs a **named, top-level function** that it can reliably import and find.\n",
    "\n",
    "#### The Core Problem: Process Isolation & Pickling\n",
    "\n",
    "1.  **Separate Processes:** `multiprocessing` creates brand-new, isolated processes with their own memory. They do not share local variables with the main process.\n",
    "2.  **Sending Instructions:** To send a task to a worker process, Python must **serialize** (or \"pickle\") the function and its arguments into a byte stream.\n",
    "\n",
    "#### `lambda` vs. `def`: The Pickling Test\n",
    "\n",
    "This is where the two types of functions fundamentally differ.\n",
    "\n",
    "| Characteristic | `lambda` (Inside a function) | `def` (Top-Level Function) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Analogy** | A temporary \"sticky note\" | A named \"book\" in a library |\n",
    "| **Scope** | Local & Anonymous | Global & Named |\n",
    "| **Picklable?** | ‚ùå **No** | ‚úÖ **Yes** |\n",
    "| **Reason** | The worker process receives instructions to find a nameless, local \"sticky note\" that doesn't exist in its own separate memory space. | The worker process is told to \"import the `module` and find the function named `unzip_worker`,\" which is a reliable, findable address. |\n",
    "\n",
    "#### The Error Message Decoded\n",
    "\n",
    "The error `AttributeError: Can't get local object ... <lambda>` is Python's direct way of saying: \"I was sent to a new process and told to find this local, anonymous function, but it doesn't exist here.\"\n",
    "\n",
    "> **The Golden Rule:** When using `multiprocessing`, any function passed as a task to a worker **must be a named function defined at the top level of a module**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pwraicore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
